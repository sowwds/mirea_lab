# Объяснение лабораторной работы: PCA и кластеризация данных NBA

## Введение

**Цель работы:** Проанализировать набор данных о баскетболистах NBA за 2013 год. Поскольку у данных много признаков (большая размерность), мы не можем просто так их визуализировать и найти в них закономерности. Поэтому мы применим:
1.  **Метод главных компонент (PCA)** для снижения размерности данных с сохранением максимума информации.
2.  **Кластеризацию (K-Means)**, чтобы сгруппировать похожих игроков в "кластеры" на основе их игровой статистики.

---

## Шаг 1: Загрузка и предобработка данных

Прежде чем применять алгоритмы, данные нужно "причесать". Этот этап критически важен.

```python
import pandas as pd
from sklearn.preprocessing import StandardScaler, LabelEncoder

# Загрузка данных
df = pd.read_csv('nba_2013.csv')

# Удаление неинформативных столбцов
df_processed = df.drop(['player', 'bref_team_id', 'season', 'season_end'], axis=1)

# Обработка пропусков
df_processed = df_processed.fillna(0)

# Кодирование категориальной переменной 'pos'
le = LabelEncoder()
y_true = le.fit_transform(df_processed['pos'])
X = df_processed.drop('pos', axis=1)

# Масштабирование данных
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)
```

### Термины и объяснения:

*   **Pandas DataFrame:** Это как "умная" таблица в Python. Мы загружаем наш CSV-файл в DataFrame, чтобы с ним было удобно работать (удалять столбцы, заполнять пропуски и т.д.).
*   **Обработка пропусков (`fillna(0)`):** В данных часто бывают пустые ячейки (пропуски). Алгоритмы машинного обучения не умеют с ними работать, поэтому мы заполняем их каким-то значением. В нашем случае — нулем.
*   **Label Encoding:** Компьютер понимает только числа. Признак `pos` (позиция игрока) содержит текст (например, 'SF', 'C', 'PF'). `LabelEncoder` преобразует эти текстовые метки в числа (например, 'SF' -> 0, 'C' -> 1 и т.д.). Мы сохраняем эти "правильные" метки, чтобы позже сравнить с результатами нашей кластеризации.
*   **Масштабирование признаков (`StandardScaler`):** Это **очень важный** шаг для PCA. У нас есть признаки с разным размахом значений (например, возраст игрока от 20 до 40, а количество очков — сотни). Если не привести их к одному масштабу, PCA решит, что признаки с большим размахом (сотни очков) важнее, чем признаки с маленьким (возраст). `StandardScaler` делает так, чтобы у каждого признака среднее значение стало 0, а стандартное отклонение — 1. Теперь все признаки "равноправны".

---

## Шаг 2: Метод главных компонент (PCA)

Теперь, когда данные готовы, мы можем снизить их размерность.

```python
from sklearn.decomposition import PCA

# Применяем PCA и строим график
pca = PCA().fit(X_scaled)
plt.plot(np.cumsum(pca.explained_variance_ratio_))
# ... (код для графика)
```

### Термины и объяснения:

*   **Метод главных компонент (PCA — Principal Component Analysis):** Представьте, что у вас есть облако точек в 28-мерном пространстве (у нас 28 признаков). PCA находит новые "оси координат" для этого облака.
    *   **Первая главная компонента** — это ось, вдоль которой данные имеют наибольший разброс (дисперсию). Это самое информативное "направление" в данных.
    *   **Вторая главная компонента** — это вторая по информативности ось, перпендикулярная первой.
    *   И так далее.
*   **Объясненная дисперсия (Explained Variance):** Это метрика, которая показывает, какой процент информации из исходных данных "содержится" в каждой главной компоненте.
*   **График кумулятивной объясненной дисперсии:** Наш код строит график, который показывает, какой процент информации мы сохраним, если возьмем 1, 2, 3... N главных компонент. Мы ищем "точку насыщения". В нашем случае, мы видим, что примерно **15 компонент** достаточно, чтобы объяснить **90%** всей информации из исходных 28 признаков. Это значит, что мы можем безболезненно "выкинуть" почти половину признаков, заменив их на 15 новых, но при этом потеряем всего 10% информации.

---

## Шаг 3: Кластеризация методом K-Means

Теперь у нас есть сжатые, но информативные данные (`X_pca`). Применим к ним алгоритм кластеризации, чтобы найти группы похожих игроков.

```python
from sklearn.cluster import KMeans

# Метод локтя для поиска оптимального k
# ... (код для графика)

# Применение K-Means с оптимальным k
optimal_k = 5
kmeans = KMeans(n_clusters=optimal_k, random_state=42)
y_kmeans = kmeans.fit_predict(X_pca)
```

### Термины и объяснения:

*   **Кластеризация:** Задача обучения без учителя, цель которой — сгруппировать объекты так, чтобы объекты в одной группе (кластере) были похожи друг на друга, а объекты из разных групп — не похожи.
*   **K-Means (метод k-средних):** Популярный алгоритм кластеризации. Мы заранее говорим ему, сколько кластеров (`k`) мы хотим найти. Он случайным образом выбирает `k` точек-центров и итеративно выполняет два шага:
    1.  Относит каждую точку данных к ближайшему центру.
    2.  Пересчитывает центр каждого кластера как среднее всех точек в нем.
    Это повторяется, пока центры не перестанут смещаться.
*   **Метод локтя:** Как выбрать правильное `k`? Мы строим график зависимости "инерции" (суммы квадратов расстояний от точек до центров их кластеров) от `k`. С увеличением `k` инерция всегда падает. Мы ищем точку, где график изгибается, как "локоть" — после нее добавление новых кластеров уже не дает такого сильного прироста качества. В нашем случае, "локоть" находится в районе `k=5`.

---

## Шаг 4: Оценка качества и визуализация

Мы разбили игроков на 5 кластеров. Насколько хорошо у нас это получилось?

```python
from sklearn.metrics import silhouette_score, adjusted_rand_score
import seaborn as sns

# Расчет метрик
silhouette = silhouette_score(X_pca, y_kmeans)
ari = adjusted_rand_score(y_true, y_kmeans)

# Визуализация
sns.scatterplot(x=X_pca[:, 0], y=X_pca[:, 1], hue=y_kmeans)
```

### Термины и объяснения:

*   **Коэффициент силуэта (Silhouette Score):** Это *внутренняя* метрика, ей не нужны правильные ответы. Она для каждой точки измеряет, насколько она близка к "своим" соседям по кластеру в сравнении с "чужими" из ближайшего кластера.
    *   Значение **близко к 1**: отлично, кластеры плотные и хорошо разделены.
    *   Значение **близко к 0**: плохо, кластеры пересекаются.
*   **Adjusted Rand Index (ARI):** Это *внешняя* метрика. Она сравнивает наши предсказанные кластеры (`y_kmeans`) с заранее известными "истинными" метками (`y_true` — позиции игроков, которые мы закодировали в шаге 1).
    *   Значение **близко к 1**: отлично, наши кластеры почти идеально совпадают с реальными позициями игроков.
    *   Значение **близко к 0**: плохо, разбиение на кластеры не имеет ничего общего с позициями игроков.
*   **Визуализация:** Мы строим диаграмму рассеяния, где по осям отложены первые две главные компоненты (самые информативные), а цвет точек соответствует номеру кластера, который нашел K-Means. Это позволяет нам "увидеть" наши многомерные данные и то, как алгоритм разделил игроков на группы.
